{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt('RegData.csv', delimiter=',')\n",
    "X =  my_data[:,[0]]\n",
    "Y =  my_data[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9000001 ],\n",
       "       [6.69999981],\n",
       "       [4.9000001 ],\n",
       "       [7.9000001 ],\n",
       "       [9.80000019],\n",
       "       [6.9000001 ],\n",
       "       [6.0999999 ],\n",
       "       [6.19999981],\n",
       "       [6.        ],\n",
       "       [5.0999999 ],\n",
       "       [4.69999981],\n",
       "       [4.4000001 ],\n",
       "       [5.80000019]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.        ],\n",
       "       [7.4000001 ],\n",
       "       [5.        ],\n",
       "       [7.19999981],\n",
       "       [7.9000001 ],\n",
       "       [6.0999999 ],\n",
       "       [6.        ],\n",
       "       [5.80000019],\n",
       "       [5.19999981],\n",
       "       [4.19999981],\n",
       "       [4.        ],\n",
       "       [4.4000001 ],\n",
       "       [5.19999981]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_h(X_h , Theta_h):\n",
    "    \"\"\"\n",
    "    this function takes the \n",
    "    features matrix (mxn) matrix\n",
    "    and the \n",
    "    theata vector (nx1) vactor\n",
    "    ----\n",
    "    return the predicted values vector (mx1)\n",
    "    \"\"\"\n",
    "    if (X_h.shape[1] == len(Theta_h)):\n",
    "        return np.dot(X_h, Theta_h)\n",
    "    else:\n",
    "        X_h = np.hstack((np.ones((X_h.shape[0],1)),X_h))\n",
    "        return np.dot(X_h, Theta_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(X_f,Y_f ,Theta_g ):\n",
    "    \"\"\"\n",
    "    this function takes the \n",
    "    features matrix (mxn) matrix\n",
    "    and the \n",
    "    theata vector (nx1) vactor\n",
    "    ------\n",
    "    it uses the function predict_h()\n",
    "    -------\n",
    "    return\n",
    "    the gradient vector (nx1) vactor\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X_f.shape[0]\n",
    "    #np.dot(X_h, Theta_h)\n",
    "    y_hat = predict_h(X_f, Theta_g)\n",
    "    loss = y_hat - Y_f\n",
    "    return (1/m)*np.dot(X_f.T,(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_criteria_gradient(grad,limits = .0001):\n",
    "    return abs(np.sum(grad)) <= limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def batchsplitter(features_, target_ , batch_size_):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Adagrad, \n",
    "    RMSProp, and \n",
    "    Adam algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum_and_NAG(theta,learning_rate , momentum_gama, full_features[i] ,full_target[i] , nag)\n",
    "def Momentum_and_NAG(theta_, learning_rate_ = 0.0001 , momentum_gama_ = 0, X_,Y_ , nag=False):\n",
    "    v = 0\n",
    "    if nag == True:\n",
    "        theta_ -= (momentum_gama_* v)\n",
    "        y_hat[i] = predict_h(X_,theta)\n",
    "        g = get_gradient(X_, X_,theta_)\n",
    "        theta_ = theta_ - (learning_rate_ *g)\n",
    "        \n",
    "    else:\n",
    "        y_hat[i] = predict_h(X_,theta_)\n",
    "        \n",
    "    #out of if else nag\n",
    "    g = get_gradient(X_,Y_ ,theta_)\n",
    "    v = momentum_gama_*v + (learning_rate_*g)\n",
    "    theta_ = theta_ - v\n",
    "    return theta_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(features , target\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 0.0001\n",
    "                     , batch_size = None\n",
    "                     , momentum_gama = 0\n",
    "                     , nag = False\n",
    "                     , useAdagrad = False\n",
    "                     , beta_rmsprob = 1\n",
    "                     , beta_mAdam = 0\n",
    "                     , beta_vAdam = 1\n",
    "                     , gradlimit = .0001):\n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features = np.hstack((np.ones((features.shape[0],1)),features))\n",
    "    \n",
    "    if (beta_mAdam != 0 or momentum_gama != 0):\n",
    "        raise Exception('You cant use both adam and the momentum bases algorithm')\n",
    "    \n",
    "    if batch_size == None:\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "     #split into batches\n",
    "    No_of_batches = math.ceil(features.shape[0]/batch_size)\n",
    "    full_features = np.array_split(features, No_of_batches)\n",
    "    full_target  = np.array_split(target, No_of_batches)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    \n",
    "    y_hat = full_target.copy()\n",
    "    #intialize v\n",
    "    v_moment = 0\n",
    "    \n",
    "    if not useAdagrad:\n",
    "        v_Adagrad = 1\n",
    "    \n",
    "        \n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    \n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        for i in range(No_of_batches):\n",
    "            #for moment and nag\n",
    "            if nag == True:\n",
    "                theta -= (momentum_gama* v_moment)\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "                theta = theta - (learning_rate *g)\n",
    "\n",
    "            else:\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "\n",
    "            #out of if else nag\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            v_moment = momentum_gama*v_moment + (learning_rate*g)\n",
    "            theta = theta - v_moment\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "        \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads= gradient_descent(X,Y, number_of_iterations=500\n",
    "                     , learning_rate = 0.001)\n",
    "iter_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
