{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as skm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt('MultipleLR.csv', delimiter=',')\n",
    "X =  my_data[:,0:-1]\n",
    "Y =  my_data[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_h(X_h , Theta_h):\n",
    "    \"\"\"\n",
    "    this function takes the \n",
    "    features matrix (mxn) matrix\n",
    "    and the \n",
    "    theata vector (nx1) vactor\n",
    "    ----\n",
    "    return the predicted values vector (mx1)\n",
    "    \"\"\"\n",
    "    if (X_h.shape[1] == len(Theta_h)):\n",
    "        return np.dot(X_h, Theta_h)\n",
    "    else:\n",
    "        X_h = np.hstack((np.ones((X_h.shape[0],1)),X_h))\n",
    "        return np.dot(X_h, Theta_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(X_f,Y_f ,Theta_g ):\n",
    "    \"\"\"\n",
    "    this function takes the \n",
    "    features matrix (mxn) matrix\n",
    "    and the \n",
    "    theata vector (nx1) vactor\n",
    "    ------\n",
    "    it uses the function predict_h()\n",
    "    -------\n",
    "    return\n",
    "    the gradient vector (nx1) vactor\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X_f.shape[0]\n",
    "    #np.dot(X_h, Theta_h)\n",
    "    y_hat = predict_h(X_f, Theta_g)\n",
    "    loss = y_hat - Y_f\n",
    "    return (1/m)*np.dot(X_f.T,(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_criteria_gradient(grad,limits = .0001):\n",
    "    return abs(np.sum(grad)) <= limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparethedata(features,target,batch_size):\n",
    "    \"\"\"\n",
    "    return \n",
    "    features\n",
    "    batch_size \n",
    "    No_of_batches \n",
    "    full_features \n",
    "    full_target\n",
    "    \"\"\"\n",
    "    features = np.hstack((np.ones((features.shape[0],1)),features))\n",
    "       \n",
    "    if batch_size == None:\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "     #split into batches\n",
    "    No_of_batches = math.ceil(features.shape[0]/batch_size)\n",
    "    full_features = np.array_split(features, No_of_batches)\n",
    "    full_target  = np.array_split(target, No_of_batches)\n",
    "    return features,batch_size , No_of_batches , full_features , full_target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Adagrad, \n",
    "    RMSProp, and \n",
    "    Adam algorithm."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Momentum_and_NAG(theta,learning_rate , momentum_gama, full_features[i] ,full_target[i] , nag)\n",
    "def Momentum_and_NAG(theta_, learning_rate_ = 0.0001 , momentum_gama_ = 0, X_,Y_ , nag=False):\n",
    "    v = 0\n",
    "    if nag == True:\n",
    "        theta_ -= (momentum_gama_* v)\n",
    "        y_hat[i] = predict_h(X_,theta)\n",
    "        g = get_gradient(X_, X_,theta_)\n",
    "        theta_ = theta_ - (learning_rate_ *g)\n",
    "        \n",
    "    else:\n",
    "        y_hat[i] = predict_h(X_,theta_)\n",
    "        \n",
    "    #out of if else nag\n",
    "    g = get_gradient(X_,Y_ ,theta_)\n",
    "    v = momentum_gama_*v + (learning_rate_*g)\n",
    "    theta_ = theta_ - v\n",
    "    return theta_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperate into methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(features , target\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 0.0001\n",
    "                     , batch_size = None\n",
    "                     , gradlimit = .0001):\n",
    "    \n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features,batch_size , No_of_batches , full_features , full_target = preparethedata(features,target,batch_size)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    y_hat = full_target.copy()\n",
    "    \n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    #loop over the iterations\n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        #loop over the batches\n",
    "        for i in range(No_of_batches):\n",
    "            \n",
    "            y_hat[i] = predict_h(full_features[i],theta)\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            theta = theta - learning_rate*g\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                #accrs\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "                #calc the cost\n",
    "                m = full_target[i].shape[0]\n",
    "                loss = y_hat[i] - full_target[i]\n",
    "                cost = 1/(2*m)*np.sum(loss**2)\n",
    "                costs.append(cost)\n",
    "                #######################\n",
    "                \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            #accrs\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            ########################\n",
    "            \n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "            #######################\n",
    "        \n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads= gradient_descent(X,Y, number_of_iterations=500\n",
    "                     , learning_rate = 0.0001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_momentom(features , target\n",
    "                     , number_of_iterations = 1000\n",
    "                     , learning_rate = 0.0001\n",
    "                     , batch_size = None\n",
    "                     , momentum_gama = 0\n",
    "                     , nag = False\n",
    "                     , gradlimit = .0001):\n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features,batch_size , No_of_batches , full_features , full_target = preparethedata(features,target,batch_size)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    y_hat = full_target.copy()\n",
    "    \n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    v_moment = 0\n",
    "    #loop over the iterations\n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        #loop over the batches\n",
    "        for i in range(No_of_batches):\n",
    "            #for moment and nag\n",
    "            if nag == True:\n",
    "                theta -= (momentum_gama* v_moment)\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "                theta = theta - (learning_rate *g)\n",
    "\n",
    "            else:\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "\n",
    "            #out of if else nag\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            v_moment = momentum_gama*v_moment+(learning_rate*g)\n",
    "            theta = theta - v_moment\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                #accrs\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "                #calc the cost\n",
    "                m = full_target[i].shape[0]\n",
    "                loss = y_hat[i] - full_target[i]\n",
    "                cost = 1/(2*m)*np.sum(loss**2)\n",
    "                costs.append(cost)\n",
    "                #######################\n",
    "                \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            #accrs\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            ########################\n",
    "            \n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "            #######################\n",
    "        \n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_momentom(X , Y\n",
    "                     , number_of_iterations = 1000\n",
    "                     , learning_rate = 0.00001\n",
    "                     , batch_size = None\n",
    "                     , momentum_gama = .9\n",
    "                     , nag = True\n",
    "                     , gradlimit = .0001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_Adagrad(features , target\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 0.0001\n",
    "                     , batch_size = None\n",
    "                     , gradlimit = .0001):\n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features,batch_size,No_of_batches,full_features,full_target=preparethedata(features,target,batch_size)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    y_hat = full_target.copy()\n",
    "\n",
    "    v_Adagrad = 0\n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    #loop over the iterations\n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        #loop over the batches\n",
    "        for i in range(No_of_batches):\n",
    "                \n",
    "            y_hat[i] = predict_h(full_features[i],theta)\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            v_Adagrad = v_Adagrad + g**2\n",
    "            theta = theta - learning_rate*g*(1/(np.sqrt(v_Adagrad)+1e-5))\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "        \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_Adagrad(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = .1\n",
    "                     , batch_size = None\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent with rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_rmsprop(features , target\n",
    "                      , number_of_iterations=1000\n",
    "                      , learning_rate = 0.0001\n",
    "                      , batch_size = None\n",
    "                      , beta_rmsprop = .98\n",
    "                      , gradlimit = .0001):\n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features,batch_size,No_of_batches,full_features,full_target=preparethedata(features,target,batch_size)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    y_hat = full_target.copy()\n",
    "\n",
    "    v_rmsprop = 0\n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    #loop over the iterations\n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        #loop over the batches\n",
    "        for i in range(No_of_batches):\n",
    "                \n",
    "            y_hat[i] = predict_h(full_features[i],theta)\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            v_rmsprop = beta_rmsprop*v_rmsprop + (1-beta_rmsprop)*g**2\n",
    "\n",
    "            theta = theta - learning_rate*g*(1/(np.sqrt(v_rmsprop)+1e-5))\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "        \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_rmsprop(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 0.001\n",
    "                     , batch_size = None\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent  with adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_adam(features , target\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 0.0001\n",
    "                     , batch_size = None\n",
    "                     , beta_mAdam = 0\n",
    "                     , beta_vAdam = 1\n",
    "                     , gradlimit = .0001):\n",
    "    thetas = []\n",
    "    costs = []\n",
    "    accrs = []\n",
    "    grads = []\n",
    "    \n",
    "    features,batch_size,No_of_batches,full_features,full_target=preparethedata(features,target,batch_size)\n",
    "    \n",
    "    #intialize the theta\n",
    "    theta = np.zeros(features.shape[1]).reshape((features.shape[1],1))\n",
    "    \n",
    "    # intialize Y hat for spliting \n",
    "    y_hat = full_target.copy()\n",
    "\n",
    "    v_adam = 0\n",
    "    m_adam = 0\n",
    "    #stop to max iter\n",
    "    iter_time = 0\n",
    "    #loop over the iterations\n",
    "    for t in range(number_of_iterations):\n",
    "        \n",
    "        #loop over the batches\n",
    "        for i in range(No_of_batches):\n",
    "                \n",
    "            y_hat[i] = predict_h(full_features[i],theta)\n",
    "            g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "            v_adam = v_adam /(1  -beta_vAdam**(t+1) )\n",
    "            m_adam = m_adam / (1- beta_mAdam**(t+1))\n",
    "            v_adam = beta_vAdam*v_adam + (1-beta_vAdam)*g**2\n",
    "            m_adam = beta_mAdam*m_adam + (1-beta_mAdam)*g\n",
    "            theta = theta - learning_rate*m_adam*(1/(np.sqrt(v_adam)+1e-5))\n",
    "            thetas.append(theta)\n",
    "    \n",
    "            # for full batch or small batches cases\n",
    "            if (batch_size >=2):\n",
    "                y_hat[i] = predict_h(full_features[i],theta)\n",
    "                accrs.append(round(skm.r2_score(full_target[i],y_hat[i]),6))\n",
    "                \n",
    "        \n",
    "        # for 1 by 1 value (Stochastic)\n",
    "        if (batch_size <2):\n",
    "            y_hat = np.dot(features, theta)\n",
    "            accrs.append(round(skm.r2_score(target,y_hat),6))\n",
    "            #calc the cost\n",
    "            m = target.shape[0]\n",
    "            y_hat = predict_h(features,theta)\n",
    "            loss = y_hat - target\n",
    "            cost = 1/(2*m)*np.sum(loss**2)\n",
    "            costs.append(cost)\n",
    "        iter_time += 1\n",
    "        \n",
    "        #get final grad after each iteration to add to list \n",
    "        g = get_gradient(full_features[i],full_target[i] ,theta)\n",
    "        grads.append(g)\n",
    "        \n",
    "        if stop_criteria_gradient(g,gradlimit):\n",
    "            break\n",
    "#     end the for iteration loop\n",
    "    \n",
    "    #for extraction purpose \n",
    "    thetas = np.array(thetas).reshape(iter_time,features.shape[1])\n",
    "    \n",
    "    #end function \n",
    "    return theta,thetas,costs, accrs,iter_time,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_adam(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = .155\n",
    "                     , batch_size = None\n",
    "                     , beta_mAdam = .9\n",
    "                     , beta_vAdam = .9\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_adam(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = .155\n",
    "                     , batch_size = None\n",
    "                     , beta_mAdam = .9\n",
    "                     , beta_vAdam = .99\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_adam(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = .155\n",
    "                     , batch_size = None\n",
    "                     , beta_mAdam = .6\n",
    "                     , beta_vAdam = .99\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta,thetas,costs, accrs,iter_time,grads = gradient_descent_with_adam(X , Y\n",
    "                     , number_of_iterations=1000\n",
    "                     , learning_rate = 1\n",
    "                     , batch_size = None\n",
    "                     , beta_mAdam = .9\n",
    "                     , beta_vAdam = .9\n",
    "                     , gradlimit = .00001)\n",
    "accrs[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
